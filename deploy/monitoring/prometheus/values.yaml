## kube-prometheus-stack values
## Ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack

## Component scraping configurations
##
prometheus:
  prometheusSpec:
    # Enable Prometheus Operator to manage Prometheus
    enableAdminAPI: false
    
    # Configure retention
    retention: 30d
    retentionSize: "50GiB"
    
    # Configure external labels
    externalLabels:
      environment: production
      team: quantum
    
    # Configure scrape intervals
    scrapeInterval: 15s
    evaluationInterval: 30s
    
    # Configure persistent storage
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: gp2
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi
    
    # Configure additional scrape configs
    additionalScrapeConfigs:
      - job_name: 'quantum-node'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            action: keep
            regex: quantum-node
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: 9090
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node
    
    # Configure alerting
    ruleSelector:
      matchLabels:
        app: kube-prometheus-stack
        release: monitoring
    
    # Enable Thanos sidecar for long-term storage
    thanos:
      image: thanosio/thanos:v0.28.0
      version: v0.28.0
      objectStorageConfig:
        key: thanos.yaml
        name: thanos-objstore-config

## Alertmanager configuration
##
alertmanager:
  enabled: true
  alertmanagerSpec:
    logLevel: info
    logFormat: json
    retention: 120h
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: gp2
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    
    # Configure alertmanager to send alerts to multiple receivers
    config:
      global:
        resolve_timeout: 5m
        http_config: {}
        smtp_smarthost: 'smtp.example.com:587'
        smtp_from: 'alertmanager@example.com'
        smtp_auth_username: 'alertmanager'
        smtp_auth_password: 'password'
        smtp_require_tls: true
      
      # Route alerts to different receivers based on labels
      route:
        group_by: ['alertname', 'severity']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 1h
        receiver: 'slack'
        routes:
          - match:
              severity: 'critical'
            receiver: 'pagerduty'
          - match:
              severity: 'warning'
            receiver: 'slack'
      
      # Define receivers
      receivers:
        - name: 'null'
        - name: 'slack'
          slack_configs:
            - api_url: 'https://hooks.slack.com/services/XXXX/XXXX/XXXX'
              channel: '#alerts'
              send_resolved: true
              title: '{{ template "slack.default.title" . }}'
              text: '{{ template "slack.default.text" . }}'
        - name: 'pagerduty'
          pagerduty_configs:
            - routing_key: 'your-pagerduty-key'
              description: '{{ .CommonAnnotations.summary }}'
              details:
                firing: '{{ .Alerts.Firing | len }}'
                resolved: '{{ .Alerts.Resolved | len }}'
                summary: '{{ .CommonAnnotations.summary }}'
                description: '{{ .CommonAnnotations.description }}'

## Grafana configuration
##
grafana:
  enabled: true
  adminPassword: "prom-operator"
  
  # Configure persistent storage
  persistence:
    enabled: true
    size: 10Gi
    storageClassName: gp2
  
  # Configure datasources
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://monitoring-prometheus-oper-prometheus.monitoring.svc:9090
          access: proxy
          isDefault: true
          jsonData:
            timeInterval: "5s"
        - name: Loki
          type: loki
          url: http://loki.monitoring.svc:3100
          access: proxy
          jsonData:
            maxLines: 1000
  
  # Configure dashboards
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      searchNamespace: ALL
      provider:
        allowUiUpdates: true
    datasources:
      enabled: true
      defaultDatasourceEnabled: true
  
  # Configure plugins
  plugins:
    - grafana-piechart-panel
    - grafana-worldmap-panel
    - natel-plotly-panel
    - vonage-status-panel
    - grafana-clock-panel
    - michaeldmoore-multistat-panel
    - grafana-piechart-panel
    - mtanda-heatmap-epoch-panel
    - neocat-cal-heatmap-panel
    - petrslavotinek-carpetplot-panel
    - briangann-gauge-panel
    - briangann-datatable-panel
    - jdbranham-diagram-panel
    - digiapulssi-breadcrumb-panel
    - digrich-bubblechart-panel
    - satelllte-3d-globe-panel
    - natel-influx-admin-panel
    - natel-plotly-panel
    - michaeldmoore-multistat-panel
    - mtanda-histogram-panel
    - mtanda-heatmap-epoch-panel
    - neocat-cal-heatmap-panel
    - petrslavotinek-carpetplot-panel
    - vonage-status-panel
    - yesoreyeram-boomtable-panel
    - yesoreyeram-boomtheme-panel
    - yesoreyeram-boomtheme-panel
    - grafana-clock-panel
    - grafana-simple-json-datasource
    - grafana-azure-monitor-datasource
    - grafana-github-datasource
    - grafana-kubernetes-app
    - grafana-serversend-datasource
    - grafana-splunk-datasource
    - grafana-x-ray-datasource
    - jasonlashua-sshcommand-datasource
    - marcusolsson-json-datasource
    - marcusolsson-treemap-panel
    - natel-discrete-panel
    - natel-plotly-panel
    - petrslavotinek-carpetplot-panel
    - ryantxu-ajax-panel
    - scadavis-synoptic-panel
    - snuids-radar-panel
    - spejss-json-datasource
    - yesoreyeram-boomtable-panel
    - yesoreyeram-boomtheme-panel
    - grafana-azure-data-explorer-datasource
    - grafana-azure-monitor-datasource
    - grafana-github-datasource
    - grafana-kubernetes-app
    - grafana-serversend-datasource
    - grafana-splunk-datasource
    - grafana-x-ray-datasource
    - jasonlashua-sshcommand-datasource
    - marcusolsson-json-datasource
    - marcusolsson-treemap-panel
    - natel-discrete-panel
    - natel-plotly-panel
    - petrslavotinek-carpetplot-panel
    - ryantxu-ajax-panel
    - scadavis-synoptic-panel
    - snuids-radar-panel
    - spejss-json-datasource
    - yesoreyeram-boomtable-panel
    - yesoreyeram-boomtheme-panel

## kube-state-metrics configuration
##
kubeStateMetrics:
  enabled: true
  image:
    repository: k8s.gcr.io/kube-state-metrics/kube-state-metrics
    tag: v2.4.2
  
  # Add additional collectors
  collectors:
    certificatesigningrequests: true
    horizontalpodautoscalers: true
    cronjobs: true
    jobs: true
    ingresses: true
    poddisruptionbudgets: true
    storageclasses: true
    volumeattachments: true
    csinodes: true
    csidrivers: true
    csiStorageCapacity: true
    runtimeclasses: true
    priorityclasses: true
    csistoragecapacities: true
    mutatingwebhookconfigurations: true
    validatingwebhookconfigurations: true

## node-exporter configuration
##
nodeExporter:
  enabled: true
  image:
    repository: prom/node-exporter
    tag: v1.3.1
  
  # Collect additional metrics
  extraArgs:
    - --collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($|/)
    - --collector.filesystem.ignored-fs-types=^(sys|proc|autofs|rootfs|overlay|aufs|squashfs|efivarfs)$
    - --collector.netclass.ignored-devices=^(veth.*|[a-f0-9]{15})$
    - --collector.netdev.device-blacklist=^(veth.*|[a-f0-9]{15})$
    - --collector.netstat.fields=^(.*_in|.*_out|.*_err|.*_drop|.*_fifo|.*_colls|.*_carrier|.*_compressed)$
    - --collector.diskstats.ignored-devices=^(ram|loop|fd|(h|s|v|xv)d[a-z]|nvme\d+n\d+p)\d+$
    - --collector.vmstat.fields=^(oom_kill|pgpg|pswp|pg.*fault).*$

## kubelet configuration
##
kubelet:
  enabled: true
  namespace: kube-system
  serviceMonitor:
    https: true
    insecureSkipVerify: true
    metricRelabelings:
      - sourceLabels: [__name__]
        regex: 'container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|memory_failures_total)'
        action: keep

## prometheus-operator configuration
##
prometheusOperator:
  enabled: true
  image:
    repository: quay.io/prometheus-operator/prometheus-operator
    tag: v0.54.1
  
  # Configure additional service monitors
  serviceMonitor:
    interval: 30s
    selfMonitor: true
    
  # Configure additional rules
  prometheusRule:
    enabled: true
    additionalLabels:
      release: monitoring
    
    # Add additional alert rules
    additionalPrometheusRules:
      - name: quantum-node-rules
        groups:
          - name: quantum-node
            rules: []  # Will be populated from quantum-alerts.yaml

## Thanos configuration
##
thanos:
  enabled: true
  objectStorageConfig:
    secretName: thanos-objstore-config
    secretKey: thanos.yaml
  
  # Configure compactor for downsampling and retention
  compactor:
    enabled: true
    retentionResolutionRaw: 30d
    retentionResolution5m: 90d
    retentionResolution1h: 1y
    consistencyDelay: 30m
    blockSyncConcurrency: 20
    compactConcurrency: 1
    compactionConcurrency: 1
    deduplicationReplicaLabel: ""
    deleteDelay: 48h
  
  # Configure store gateway for querying historical data
  storegateway:
    enabled: true
    replicaCount: 2
  
  # Configure query frontend for better query performance
  queryFrontend:
    enabled: true
    replicaCount: 2
  
  # Configure query for querying metrics
  query:
    enabled: true
    replicaCount: 2
    dnsDiscovery:
      sidecarsService: "monitoring-prometheus-operated"
      sidecarsNamespace: "monitoring"
